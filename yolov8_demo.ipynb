{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import cv2\n",
    "    from ultralytics import YOLO\n",
    "    from ultralytics.yolo.v8.detect.predict import DetectionPredictor\n",
    "    import torchvision.ops as ops\n",
    "    import colorsys\n",
    "    import datetime\n",
    "    import time\n",
    "    import mediapipe as mp\n",
    "    from moviepy.editor import VideoFileClip\n",
    "    import numpy as np\n",
    "    \n",
    "    print('All packages imported')\n",
    "except:\n",
    "    !pip install cv2 ultralytics torchvision moviepy mediapipe numpy\n",
    "    import cv2\n",
    "    from ultralytics import YOLO\n",
    "    from ultralytics.yolo.v8.detect.predict import DetectionPredictor\n",
    "    import torchvision.ops as ops\n",
    "    import colorsys\n",
    "    import datetime\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import mediapipe as mp\n",
    "    from moviepy.editor import VideoFileClip\n",
    "    print('Some packages were not installed, installed and imported')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resize input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code that converts the resolution of a video to 800x400:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path, width, height):\n",
    "    video = cv2.VideoCapture(input_path)\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        raise ValueError(\"Kan de video niet lezen\")\n",
    "\n",
    "    # Krijg de oorspronkelijke breedte en hoogte van de video\n",
    "    original_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Bereken de schaalverhouding\n",
    "    scale_ratio = min(width / original_width, height / original_height)\n",
    "\n",
    "    # Bereken het nieuwe formaat\n",
    "    new_width = int(original_width * scale_ratio)\n",
    "    new_height = int(original_height * scale_ratio)\n",
    "\n",
    "    # Maak een VideoWriter-object om het uitvoerbestand te maken\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, 30.0, (new_width, new_height))\n",
    "\n",
    "    while success:\n",
    "        # Verklein het frame naar het nieuwe formaat\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        # Schrijf het verkleinde frame naar het uitvoerbestand\n",
    "        output_video.write(resized_frame)\n",
    "\n",
    "        # Lees het volgende frame\n",
    "        success, frame = video.read()\n",
    "\n",
    "    # Sluit de video-objecten\n",
    "    video.release()\n",
    "    output_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pad naar het invoerbestand\n",
    "# input_file = 'vid.mp4'\n",
    "\n",
    "# # Pad naar het uitvoerbestand\n",
    "# output_file = 'vid_res.mp4'\n",
    "\n",
    "# # Breedte en hoogte voor het formaat wijzigen\n",
    "# target_width = 640\n",
    "# target_height = 360\n",
    "\n",
    "# resize_video(input_file, output_file, target_width, target_height)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply the model on the converted video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSEVIDEO = \"videos/posevideo.mp4\"  # video\n",
    "PADDLEVIDEO = \"videos/vid1.mp4\"  # video\n",
    "threshold = 0.50 # detection threshold\n",
    "model = YOLO('models/paddletracker v1.7.pt')\n",
    "\n",
    "# Mediapipe utils\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pose analysis done by Google Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid \n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_analysis(frame):\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        # Make detection\n",
    "        results = pose.process(frame)\n",
    "\n",
    "        stance = \"Stance not OK\"\n",
    "\n",
    "        # Recolor back to BGR\n",
    "        frame.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Get coordinates\n",
    "            elbow_left = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            shoulder_left = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            hip_left = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            wrist_left = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "\n",
    "            elbow_right = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            shoulder_right = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            hip_right = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "\n",
    "            # Calculate angles\n",
    "            angle_hipshoulderelbow_left = calculate_angle(hip_left, shoulder_left, elbow_left)\n",
    "            angle_hipshoulderelbow_right = calculate_angle(hip_right, shoulder_right, elbow_right)\n",
    "\n",
    "\n",
    "            # Visualize angles\n",
    "            cv2.putText(frame, str(angle_hipshoulderelbow_left),\n",
    "                        tuple(np.multiply(shoulder_left, [640, 480]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(frame, str(angle_hipshoulderelbow_right),\n",
    "                        tuple(np.multiply(shoulder_right, [640, 480]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Curl counter logic\n",
    "            if angle_hipshoulderelbow_left > 60:\n",
    "                if angle_hipshoulderelbow_right > 60:\n",
    "                    stance = \"Stance OK\"\n",
    "                else:\n",
    "                    stance = \"Right hit\"\n",
    "            elif angle_hipshoulderelbow_right > 60:\n",
    "                if angle_hipshoulderelbow_left > 60:\n",
    "                    stance = \"Stance OK\"\n",
    "                else:\n",
    "                    stance = \"Left hit\"\n",
    "                \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return [time.time(), stance]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paddle detection (incl. lights) done by Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Paddle\", \"Light_Green\", \"Other class\"]  # Voeg hier de namen van de klassen toe in de juiste volgorde\n",
    "\n",
    "def paddle_analysis(frame):\n",
    "    result = model.predict(frame, threshold)\n",
    "    result = list(result)  # Convert to a list\n",
    "    boxes = result[0].boxes.xyxy.cuda()\n",
    "    scores = result[0].boxes.conf.cuda()\n",
    "    class_ids = result[0].names\n",
    "\n",
    "    current_timestamp = time.time()\n",
    "    detected = False\n",
    "\n",
    "    image = frame.copy()  # Create a copy of the frame\n",
    "\n",
    "    for box, score, class_id in zip(boxes, scores, class_ids):\n",
    "        if score >= threshold:\n",
    "            box = [int(i) for i in box]\n",
    "\n",
    "            if class_id < 2:\n",
    "                # Draw bounding box on the image\n",
    "                cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "                # Add class label to the bounding box\n",
    "                class_name = class_names[class_id]  # Get the class name from the list\n",
    "                label = f\"{class_name}: {score:.2f}\"\n",
    "                cv2.putText(image, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            if class_id == 1:\n",
    "                detected = True\n",
    "\n",
    "    # Show the image with bounding boxes\n",
    "    cv2.imshow(\"Frame\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if detected:\n",
    "        return [current_timestamp, 'legal_hit']\n",
    "    else:\n",
    "        return [current_timestamp, 'illegal_hit']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def paddle_analysis(frame):\n",
    "#     result = model.predict(frame, threshold)\n",
    "#     result = list(result)  # Convert to a list\n",
    "#     boxes = result[0].boxes.xyxy.cuda()\n",
    "#     scores = result[0].boxes.conf.cuda()\n",
    "#     class_ids = result[0].names\n",
    "\n",
    "#     current_timestamp = time.time()\n",
    "#     detected = False\n",
    "\n",
    "#     for box, score, class_id in zip(boxes, scores, class_ids):\n",
    "#         if score >= threshold:\n",
    "#             box = [int(i) for i in box]\n",
    "\n",
    "#             if class_id == 1:\n",
    "#                 detected = True\n",
    "\n",
    "#     if detected:\n",
    "#         return [current_timestamp, 'legal_hit']\n",
    "#     else:\n",
    "#         return [current_timestamp, 'illegal_hit']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that analyses the given video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def video_analysis(cap, model):\n",
    "    prev_timestamp = time.time()\n",
    "    paddleState = []\n",
    "    poseState = []\n",
    "\n",
    "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret or frame is None or frame.shape[0] == 0 or frame.shape[1] == 0:\n",
    "                break\n",
    "\n",
    "            # Process every 5 frames\n",
    "            if frame_count % 5 == 0:\n",
    "                if model == 'pose':\n",
    "                    poseState.append(pose_analysis(frame))\n",
    "                elif model == 'paddle':\n",
    "                    paddleState.append(paddle_analysis(frame))\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid model: {model}\")\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            current_timestamp = time.time()\n",
    "            time_delta = current_timestamp - prev_timestamp\n",
    "            prev_timestamp = current_timestamp\n",
    "\n",
    "    cap.release()\n",
    "    return [paddleState, poseState]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that takes two video path's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_videos(INPUT1, INPUT2):\n",
    "    \n",
    "    video1 = cv2.VideoCapture(INPUT1)\n",
    "    video2 = cv2.VideoCapture(INPUT2)\n",
    "\n",
    "    result = video_analysis(video1, 'pose')\n",
    "    # result = video_analysis(video2, 'paddle')\n",
    "\n",
    "    for i in result:\n",
    "        for j in i:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 127.0ms\n",
      "Speed: 4.5ms preprocess, 127.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 275.3ms\n",
      "Speed: 5.6ms preprocess, 275.3ms inference, 3.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 10.6ms\n",
      "Speed: 4.0ms preprocess, 10.6ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 10.5ms\n",
      "Speed: 3.5ms preprocess, 10.5ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 10.5ms\n",
      "Speed: 3.0ms preprocess, 10.5ms inference, 2.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 9.5ms\n",
      "Speed: 3.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 17.1ms\n",
      "Speed: 5.0ms preprocess, 17.1ms inference, 4.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 23.0ms\n",
      "Speed: 3.7ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 23.0ms\n",
      "Speed: 4.6ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 28.5ms\n",
      "Speed: 4.0ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 29.6ms\n",
      "Speed: 4.0ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 89.1ms\n",
      "Speed: 5.0ms preprocess, 89.1ms inference, 16.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 80.5ms\n",
      "Speed: 4.0ms preprocess, 80.5ms inference, 5.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 54.1ms\n",
      "Speed: 4.5ms preprocess, 54.1ms inference, 6.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 10.5ms\n",
      "Speed: 3.0ms preprocess, 10.5ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 12.0ms\n",
      "Speed: 4.0ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 11.6ms\n",
      "Speed: 4.0ms preprocess, 11.6ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 11.5ms\n",
      "Speed: 3.0ms preprocess, 11.5ms inference, 3.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 12.0ms\n",
      "Speed: 4.5ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 11.5ms\n",
      "Speed: 3.5ms preprocess, 11.5ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 20.6ms\n",
      "Speed: 4.6ms preprocess, 20.6ms inference, 4.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 26.0ms\n",
      "Speed: 5.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 24.0ms\n",
      "Speed: 4.5ms preprocess, 24.0ms inference, 3.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 30.0ms\n",
      "Speed: 4.0ms preprocess, 30.0ms inference, 2.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 108.2ms\n",
      "Speed: 4.0ms preprocess, 108.2ms inference, 5.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 56.0ms\n",
      "Speed: 7.6ms preprocess, 56.0ms inference, 3.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 54.1ms\n",
      "Speed: 4.6ms preprocess, 54.1ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 1 light_green, 1 light_neutral, 1 paddle, 57.4ms\n",
      "Speed: 4.5ms preprocess, 57.4ms inference, 4.7ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 50.0ms\n",
      "Speed: 4.0ms preprocess, 50.0ms inference, 2.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 9.5ms\n",
      "Speed: 5.0ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 9.5ms\n",
      "Speed: 5.0ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 23.0ms\n",
      "Speed: 4.5ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 19.0ms\n",
      "Speed: 3.5ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 73.5ms\n",
      "Speed: 4.6ms preprocess, 73.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 58.5ms\n",
      "Speed: 4.5ms preprocess, 58.5ms inference, 3.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 52.5ms\n",
      "Speed: 7.5ms preprocess, 52.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 50.5ms\n",
      "Speed: 6.6ms preprocess, 50.5ms inference, 5.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 69.1ms\n",
      "Speed: 4.7ms preprocess, 69.1ms inference, 5.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 10.5ms\n",
      "Speed: 4.0ms preprocess, 10.5ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 14.0ms\n",
      "Speed: 4.5ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 11.0ms\n",
      "Speed: 4.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 88.2ms\n",
      "Speed: 5.0ms preprocess, 88.2ms inference, 5.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 53.5ms\n",
      "Speed: 4.5ms preprocess, 53.5ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 47.1ms\n",
      "Speed: 4.0ms preprocess, 47.1ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 49.0ms\n",
      "Speed: 3.5ms preprocess, 49.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 11.5ms\n",
      "Speed: 3.6ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 9.5ms\n",
      "Speed: 4.5ms preprocess, 9.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 10.0ms\n",
      "Speed: 4.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 88.1ms\n",
      "Speed: 4.5ms preprocess, 88.1ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 218.3ms\n",
      "Speed: 17.7ms preprocess, 218.3ms inference, 3.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 480x800 2 light_neutrals, 1 paddle, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n"
     ]
    }
   ],
   "source": [
    "analyse_videos(POSEVIDEO, PADDLEVIDEO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
